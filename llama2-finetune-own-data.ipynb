{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FuXIFTFapAMI"
      },
      "outputs": [],
      "source": [
        "# Core ML libraries\n",
        "# torch>=2.0.0\n",
        "!pip install transformers>=4.30.0\n",
        "!pip install datasets>=2.10.0\n",
        "!pip install accelerate>=0.20.0\n",
        "\n",
        "# Data processing\n",
        "!pip install pandas>=1.5.0\n",
        "!pip install numpy>=1.24.0\n",
        "!pip install scikit-learn>=1.3.0\n",
        "!pip install openpyxl>=3.1.0\n",
        "\n",
        "# Utilities\n",
        "!pip install tqdm>=4.65.0\n",
        "!pip install wandb>=0.15.0  # Optional: for experiment tracking\n",
        "!pip install tensorboard>=2.13.0  # Optional: for logging\n",
        "\n",
        "# Additional useful packages\n",
        "!pip install peft>=0.4.0  # For parameter-efficient fine-tuning (LoRA)\n",
        "!pip install bitsandbytes>=0.39.0  # For 8-bit training\n",
        "!pip install sentencepiece>=0.1.99  # For some tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "\n",
        "from transformers import (\n",
        "    DataCollatorWithPadding,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig # For potential quantization\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional\n",
        "import warnings\n",
        "import os\n",
        "from accelerate import Accelerator # For distributed training / mixed precision / device handling\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training # For LoRA"
      ],
      "metadata": {
        "id": "KkI-X4PaloCh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure this path points to your Excel file\n",
        "EXCEL_FILE_PATH = \"./Resource-Skills-Experience-Data2.xlsx\"\n",
        "BASE_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\" # Or any other suitable model\n",
        "OUTPUT_DIR = \"./output/finetuned_resource_model_cuda\" # Changed output dir name\n",
        "LOGGING_DIR = \"./logs/finetuning_logs_cuda\"\n",
        "CHECKPOINT_DIR = \"./checkpoint/finetuning_checkpoints_cuda\"\n",
        "MAX_LENGTH = 512\n",
        "TEST_SIZE = 0.1\n",
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "0dbwDuAGl6xX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available. Found {torch.cuda.device_count()} GPU(s).\")\n",
        "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "    # Set default device for tensors if not using Accelerator's context\n",
        "    # torch.set_default_device(\"cuda\") # Generally handled by Accelerator/Trainer\n",
        "else:\n",
        "    print(\"CUDA is not available. Training will run on CPU (which will be very slow).\")\n",
        "    print(\"Ensure you have an NVIDIA GPU, appropriate drivers, and PyTorch with CUDA support installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPGd9t7bn6oW",
        "outputId": "ddeff40d-a6fd-4f92-a88a-92667a12fe46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Found 1 GPU(s).\n",
            "Current CUDA device: 0\n",
            "Device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator = Accelerator()\n",
        "print(f\"Accelerator initialized. Using device: {accelerator.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DHuradfov20",
        "outputId": "eb619b1d-d509-4de4-fa55-e746e00b6d7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accelerator initialized. Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_ARGS_CONFIG = {\n",
        "    \"output_dir\": CHECKPOINT_DIR,\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"per_device_train_batch_size\": 2, # Adjust based on your GPU memory\n",
        "    \"per_device_eval_batch_size\": 2,  # Adjust based on your GPU memory\n",
        "    \"gradient_accumulation_steps\": 8, # Effective batch size = batch_size * grad_accum * num_gpus\n",
        "    \"eval_strategy\": \"epoch\",\n",
        "    \"save_strategy\": \"epoch\",\n",
        "    \"logging_dir\": LOGGING_DIR,\n",
        "    \"logging_steps\": 10,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"warmup_steps\": 200,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"save_total_limit\": 2,\n",
        "    \"load_best_model_at_end\": True,\n",
        "    \"metric_for_best_model\": \"eval_loss\",\n",
        "    \"greater_is_better\": False,\n",
        "    # --- Mixed Precision ---\n",
        "    # Set fp16=True for mixed-precision training on most NVIDIA GPUs.\n",
        "    # Set bf16=True for Ampere GPUs (e.g., A100, RTX 30xx/40xx) or newer for potentially better stability.\n",
        "    # Accelerator/Trainer will handle the backend.\n",
        "    \"fp16\": torch.cuda.is_available(), # Enable fp16 only if CUDA is available\n",
        "    \"bf16\": False, # Set to True if you have Ampere/Hopper GPU and want to use bfloat16\n",
        "    # -----------------------\n",
        "    \"report_to\": [\"tensorboard\"], # Add \"wandb\" if you have it configured\n",
        "    \"gradient_checkpointing\": True, # Saves memory during training, might slow down slightly\n",
        "    \"optim\": \"adamw_torch\", # Recommended optimizer\n",
        "    # If using multiple GPUs, Trainer handles distribution automatically via Accelerator.\n",
        "    # \"fsdp\": \"full_shard auto_wrap\", # Example for Fully Sharded Data Parallel (requires accelerate config)\n",
        "    # \"fsdp_transformer_layer_cls_to_wrap\": [\"MistralDecoderLayer\"], # Specify layer class for FSDP wrapping (model specific)\n",
        "}\n"
      ],
      "metadata": {
        "id": "bPxHQ6dMo0kV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_LORA = True # Set to False to disable LoRA (full fine-tuning)\n",
        "LORA_CONFIG = LoraConfig(\n",
        "    r=8, # Rank of the update matrices (higher rank = more parameters, potentially better fit but more memory)\n",
        "    lora_alpha=16, # LoRA scaling factor\n",
        "    # Target modules depend on the model architecture.\n",
        "    # For Mistral: q_proj, k_proj, v_proj, o_proj are common targets in attention layers.\n",
        "    # You might need to inspect the model architecture (`print(model)`) to confirm.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05, # Dropout probability for LoRA layers\n",
        "    bias=\"none\", # Usually set to \"none\" for LoRA\n",
        "    task_type=\"CAUSAL_LM\" # Specifies the task type for PEFT\n",
        ")"
      ],
      "metadata": {
        "id": "4vrQQzhEo4Qz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_QUANTIZATION = True # Set to False to disable quantization\n",
        "# Ensure `bitsandbytes` is installed: pip install bitsandbytes\n",
        "QUANTIZATION_CONFIG = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # Load model in 4-bit precision\n",
        "    bnb_4bit_quant_type=\"nf4\", # Use NF4 (NormalFloat4) quantization type\n",
        "    # Compute dtype: Use bfloat16 for Ampere/newer GPUs, float16 for older GPUs.\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
        "    bnb_4bit_use_double_quant=True, # Use double quantization for extra memory savings\n",
        ") if USE_QUANTIZATION else None # Set to None if not using quantization\n"
      ],
      "metadata": {
        "id": "gpDl8Sfxo73c"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResourceSkillsDataset(Dataset):\n",
        "    \"\"\"Custom PyTorch Dataset for resource skills fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, data: List[Dict], tokenizer, max_length: int = MAX_LENGTH):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        print(f\"Initialized dataset with {len(data)} examples.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        # Expecting pre-formatted prompt string in the 'prompt' key\n",
        "        text = item[\"prompt\"]\n",
        "\n",
        "        # Tokenize the formatted prompt\n",
        "        # Padding is handled by the DataCollator, so we don't pad here initially.\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\", # Let DataCollator handle padding\n",
        "            return_tensors=\"pt\", # Return lists of IDs, not tensors yet\n",
        "        )\n",
        "\n",
        "        # For Causal LM, labels are usually the same as input_ids.\n",
        "        # The model learns to predict the next token.\n",
        "        # The loss calculation typically ignores padding tokens and potentially input prompt tokens.\n",
        "        # DataCollatorForLanguageModeling handles this masking if mlm=False.\n",
        "        labels = encoding[\"input_ids\"].clone()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100 # Ignore padding tokens\n",
        "\n",
        "        # Return dictionary compatible with Trainer\n",
        "        # Tensors will be created and moved to the correct device by the Trainer/DataCollator.\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"],\n",
        "            \"attention_mask\": encoding[\"attention_mask\"],\n",
        "            \"labels\": labels,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ASpP1NhtpF0I"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResourceSkillsFineTuner:\n",
        "    \"\"\"Encapsulates the fine-tuning pipeline steps.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = BASE_MODEL_NAME, use_lora: bool = USE_LORA, quantization_config: Optional[BitsAndBytesConfig] = QUANTIZATION_CONFIG):\n",
        "        self.model_name = model_name\n",
        "        self.use_lora = use_lora\n",
        "        self.quantization_config = quantization_config\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        # Use the global accelerator instance\n",
        "        self.accelerator = accelerator\n",
        "\n",
        "    def load_excel_data(self, file_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Loads data from the specified Excel file.\"\"\"\n",
        "        print(f\"Loading Excel file from: {file_path}\")\n",
        "        if not os.path.exists(file_path):\n",
        "             raise FileNotFoundError(f\"Error: Excel file not found at {file_path}\")\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            print(f\"Loaded {len(df)} records. Columns: {df.columns.tolist()}\")\n",
        "            # Clean column names (remove leading/trailing spaces)\n",
        "            df.columns = df.columns.str.strip()\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing Excel file: {e}\")\n",
        "            raise\n",
        "\n",
        "    def clean_text(self, text: Any) -> str:\n",
        "        \"\"\"Cleans and normalizes text fields from the DataFrame.\"\"\"\n",
        "        if pd.isna(text) or text == \"\":\n",
        "            return \"N/A\"\n",
        "        text = str(text).strip()\n",
        "        text = re.sub(r\"\\s+\", \" \", text) # Consolidate whitespace\n",
        "        # Keep alphanumeric, common punctuation, and symbols relevant to data\n",
        "        text = re.sub(r\"[^\\w\\s,.!?\\-:/()$%+]\", \"\", text)\n",
        "        text = re.sub(r\"(N/A\\s*)+|,\\s*N/A\", \"N/A\", text, flags=re.IGNORECASE) # Consolidate N/A\n",
        "        text = text.replace(\";\", \",\") # Standardize list separators\n",
        "        return text.strip()\n",
        "\n",
        "    def format_instruction_prompt(self, instruction: str, response: str) -> str:\n",
        "        \"\"\"Formats a prompt according to the Mistral Instruct template.\"\"\"\n",
        "        # Format: <s>[INST] Instruction [/INST] Response</s>\n",
        "        # Ensure EOS token `</s>` is correctly handled by tokenizer/training.\n",
        "        return f\"<s>[INST] {instruction.strip()} [/INST] {response.strip()}</s>\"\n",
        "\n",
        "    def create_training_prompts(self, df: pd.DataFrame) -> List[Dict[str, str]]:\n",
        "        \"\"\"Converts DataFrame rows into structured instruction prompts for fine-tuning.\"\"\"\n",
        "        print(\"Creating training prompts from DataFrame...\")\n",
        "        training_data = []\n",
        "        expected_columns = [\n",
        "            'Resource ID', 'Name', 'Job Title/Role', 'Job Title/Role Group',\n",
        "            'Org Unit L3', 'Org Unit L4', 'Country', 'Worker Type', 'Manager Name',\n",
        "            'Industry experience (years)', 'Total Experience (years)', 'Degree',\n",
        "            'Languages', 'Therapeutic Area', 'Speciality', 'Functional Expertise',\n",
        "            'Technical Skills', 'Certifications', 'Key Strengths',\n",
        "            'Current Allocation (%)', 'Availability (%)', 'On Bench'\n",
        "        ]\n",
        "\n",
        "        # Check for missing columns\n",
        "        missing_cols = [col for col in expected_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"Warning: Missing expected columns in Excel file: {missing_cols}\")\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            # Use .get(col, default) for robustness against missing columns\n",
        "            resource_id = self.clean_text(row.get('Resource ID', f'unknown_{idx}'))\n",
        "            name = self.clean_text(row.get('Name', 'Unknown'))\n",
        "\n",
        "            # --- 1. Resource Profile Prompt ---\n",
        "            try:\n",
        "                instruction = f\"Generate a profile summary for resource {name} (ID: {resource_id}).\"\n",
        "                response_parts = [\n",
        "                    f\"Name: {name}\",\n",
        "                    f\"Resource ID: {resource_id}\",\n",
        "                    f\"Role: {self.clean_text(row.get('Job Title/Role', 'N/A'))}\",\n",
        "                    f\"Role Group: {self.clean_text(row.get('Job Title/Role Group', 'N/A'))}\",\n",
        "                    f\"Department: {self.clean_text(row.get('Org Unit L3', 'N/A'))}\",\n",
        "                    f\"Team: {self.clean_text(row.get('Org Unit L4', 'N/A'))}\",\n",
        "                    f\"Location: {self.clean_text(row.get('Country', 'N/A'))}\",\n",
        "                    f\"Employment Type: {self.clean_text(row.get('Worker Type', 'N/A'))}\",\n",
        "                    f\"Reports to: {self.clean_text(row.get('Manager Name', 'N/A'))}\",\n",
        "                    f\"Industry Experience: {self.clean_text(row.get('Industry experience (years)', 0))} years\",\n",
        "                    f\"Total Experience: {self.clean_text(row.get('Total Experience (years)', 0))} years\",\n",
        "                    f\"Education: {self.clean_text(row.get('Degree', 'N/A'))}\",\n",
        "                ]\n",
        "                # Filter out N/A or zero experience entries for a cleaner response\n",
        "                response = \"\\n\".join(part for part in response_parts if \"N/A\" not in part and \"0 years\" not in part)\n",
        "                if response: # Only add if there's meaningful content\n",
        "                    training_data.append({\n",
        "                        'prompt': self.format_instruction_prompt(instruction, response),\n",
        "                        'type': 'profile',\n",
        "                        'resource_id': resource_id\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error creating profile prompt for row {idx} (ID: {resource_id}): {e}\")\n",
        "\n",
        "            # --- 2. Skills & Expertise Prompt ---\n",
        "            try:\n",
        "                instruction = f\"Detail the skills and expertise of resource {name} (ID: {resource_id}).\"\n",
        "                response_parts = [\n",
        "                    f\"Languages Spoken: {self.clean_text(row.get('Languages', 'N/A'))}\",\n",
        "                    f\"Therapeutic Areas of Focus: {self.clean_text(row.get('Therapeutic Area', 'N/A'))}\",\n",
        "                    f\"Specialized Areas: {self.clean_text(row.get('Speciality', 'N/A'))}\",\n",
        "                    f\"Functional Skills: {self.clean_text(row.get('Functional Expertise', 'N/A'))}\",\n",
        "                    f\"Technical Proficiencies: {self.clean_text(row.get('Technical Skills', 'N/A'))}\",\n",
        "                    f\"Relevant Certifications: {self.clean_text(row.get('Certifications', 'N/A'))}\",\n",
        "                    f\"Identified Key Strengths: {self.clean_text(row.get('Key Strengths', 'N/A'))}\",\n",
        "                ]\n",
        "                response = \"\\n\".join(part for part in response_parts if \"N/A\" not in part)\n",
        "                if response:\n",
        "                    training_data.append({\n",
        "                        'prompt': self.format_instruction_prompt(instruction, response),\n",
        "                        'type': 'skills',\n",
        "                        'resource_id': resource_id\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error creating skills prompt for row {idx} (ID: {resource_id}): {e}\")\n",
        "\n",
        "            # --- 3. Availability & Allocation Prompt ---\n",
        "            try:\n",
        "                instruction = f\"Report the current allocation and availability status for {name} (ID: {resource_id}).\"\n",
        "                response_parts = [\n",
        "                    f\"Current Project Allocation: {self.clean_text(row.get('Current Allocation (%)', 'N/A'))}%\",\n",
        "                    f\"Current Availability: {self.clean_text(row.get('Availability (%)', 'N/A'))}%\",\n",
        "                    f\"Currently On Bench: {self.clean_text(row.get('On Bench', 'N/A'))}\",\n",
        "                    # Add future allocation if data exists, e.g.:\n",
        "                    # f\"Next Quarter Allocation Forecast: {self.clean_text(row.get('Next 3M Allocation (%)', 'N/A'))}%\"\n",
        "                ]\n",
        "                response = \"\\n\".join(part for part in response_parts if \"N/A\" not in part and \"N/A%\" not in part)\n",
        "                if response:\n",
        "                    training_data.append({\n",
        "                        'prompt': self.format_instruction_prompt(instruction, response),\n",
        "                        'type': 'availability',\n",
        "                        'resource_id': resource_id\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error creating availability prompt for row {idx} (ID: {resource_id}): {e}\")\n",
        "\n",
        "            # --- 4. Project Matching Query Prompt (Example) ---\n",
        "            # Creates a hypothetical query based on the resource's own data for training.\n",
        "            try:\n",
        "                # Use first listed skill/role/exp if available\n",
        "                skills = self.clean_text(row.get('Technical Skills', '')).split(',')\n",
        "                required_skill = skills[0].strip() if skills and skills[0].strip() else None\n",
        "                required_role = self.clean_text(row.get('Job Title/Role', ''))\n",
        "                required_exp_str = self.clean_text(row.get('Industry experience (years)', '0'))\n",
        "                required_exp = float(required_exp_str) if required_exp_str.replace('.', '', 1).isdigit() else 0\n",
        "\n",
        "                if required_skill and required_role and required_exp > 0:\n",
        "                    instruction = f\"Evaluate if resource {name} (ID: {resource_id}) is a potential match for a project needing a '{required_role}' with expertise in '{required_skill}' and over {int(required_exp)-1} years of industry experience.\"\n",
        "\n",
        "                    # Simple positive affirmation based on the source data\n",
        "                    response = f\"Based on available data, {name} appears suitable. Key matching attributes: Role ({required_role}), Skill ({required_skill}), Industry Experience ({required_exp:.1f} years).\"\n",
        "\n",
        "                    training_data.append({\n",
        "                        'prompt': self.format_instruction_prompt(instruction, response),\n",
        "                        'type': 'matching_positive',\n",
        "                        'resource_id': resource_id\n",
        "                    })\n",
        "                # Optional: Add negative examples here if needed for robustness\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error creating matching prompt for row {idx} (ID: {resource_id}): {e}\")\n",
        "\n",
        "        print(f\"Generated {len(training_data)} raw training examples.\")\n",
        "        # Deduplicate based on the exact prompt string\n",
        "        unique_prompts = {item['prompt']: item for item in training_data}.values()\n",
        "        final_data = list(unique_prompts)\n",
        "        print(f\"Returning {len(final_data)} unique training examples after deduplication.\")\n",
        "        return final_data\n",
        "\n",
        "    def initialize_model_and_tokenizer(self):\n",
        "        \"\"\"Loads the tokenizer and model, applying quantization and LoRA if configured.\"\"\"\n",
        "        print(f\"Loading tokenizer: {self.model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
        "\n",
        "        # Set padding token if missing (common for Llama/Mistral based models)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            print(f\"Tokenizer pad_token set to eos_token ({self.tokenizer.eos_token})\")\n",
        "        # Ensure padding side is right for Causal LMs during generation/training\n",
        "        self.tokenizer.padding_side = \"right\"\n",
        "\n",
        "        print(f\"Loading model: {self.model_name}\")\n",
        "        model_load_kwargs = {\n",
        "            'trust_remote_code': True,\n",
        "            # Use Accelerator's device map for potentially multi-GPU setups or quantization\n",
        "            # `device_map=\"auto\"` lets Accelerate handle placement.\n",
        "            # For quantization with single GPU, `device_map={'': accelerator.local_process_index}` is often used.\n",
        "            'device_map': \"auto\",\n",
        "        }\n",
        "\n",
        "        if self.quantization_config:\n",
        "            print(\"Applying quantization config...\")\n",
        "            model_load_kwargs['quantization_config'] = self.quantization_config\n",
        "            # Note: `torch_dtype` might be needed depending on model/quantization interaction\n",
        "            # model_load_kwargs['torch_dtype'] = torch.bfloat16 # or torch.float16\n",
        "        else:\n",
        "             # If not quantizing, explicitly set dtype for potential mixed precision\n",
        "             compute_dtype = torch.bfloat16 if TRAINING_ARGS_CONFIG.get(\"bf16\", False) else torch.float16\n",
        "             model_load_kwargs['torch_dtype'] = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "        try:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                **model_load_kwargs\n",
        "            )\n",
        "        except Exception as e:\n",
        "             print(f\"Error loading model: {e}\")\n",
        "             print(\"Ensure you have enough GPU memory and necessary libraries (transformers, accelerate, bitsandbytes if quantizing).\")\n",
        "             raise\n",
        "\n",
        "        # Prepare model for k-bit training if quantization is enabled\n",
        "        if self.quantization_config:\n",
        "             print(\"Preparing model for k-bit training (post-quantization setup)...\")\n",
        "             # This adapts the quantized model for training, often involving gradient checkpointing setup\n",
        "             self.model = prepare_model_for_kbit_training(self.model, use_gradient_checkpointing=TRAINING_ARGS_CONFIG.get(\"gradient_checkpointing\", False))\n",
        "\n",
        "        # Apply LoRA if enabled\n",
        "        if self.use_lora:\n",
        "            print(\"Applying LoRA configuration...\")\n",
        "            self.model = get_peft_model(self.model, LORA_CONFIG)\n",
        "            print(\"LoRA enabled. Trainable parameters:\")\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "        # Ensure model is on the correct device (Accelerator usually handles this with device_map=\"auto\")\n",
        "        # print(f\"Model loaded onto device(s): {self.model.device}\") # Can be complex with device_map\n",
        "\n",
        "        print(\"Model and tokenizer initialized successfully.\")\n",
        "\n",
        "    def prepare_datasets(self, training_data: List[Dict]):\n",
        "        \"\"\"Splits data into train/validation sets and creates Dataset objects.\"\"\"\n",
        "        if not training_data:\n",
        "             raise ValueError(\"Cannot prepare datasets: No training data provided.\")\n",
        "\n",
        "        print(f\"Splitting {len(training_data)} examples into train/validation sets (Test size: {TEST_SIZE})...\")\n",
        "        train_data, val_data = train_test_split(\n",
        "            training_data,\n",
        "            test_size=TEST_SIZE,\n",
        "            random_state=RANDOM_STATE,\n",
        "            # Stratify if prompts have meaningful 'type' distribution, otherwise not needed\n",
        "            # stratify=[item['type'] for item in training_data]\n",
        "        )\n",
        "        print(f\"Train set size: {len(train_data)}, Validation set size: {len(val_data)}\")\n",
        "\n",
        "        train_dataset = ResourceSkillsDataset(train_data, self.tokenizer, max_length=MAX_LENGTH)\n",
        "        val_dataset = ResourceSkillsDataset(val_data, self.tokenizer, max_length=MAX_LENGTH)\n",
        "\n",
        "        return train_dataset, val_dataset\n",
        "\n",
        "    def train(self, train_dataset: Dataset, val_dataset: Dataset):\n",
        "        \"\"\"Configures and executes the training loop using the Hugging Face Trainer.\"\"\"\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            raise RuntimeError(\"Model and tokenizer must be initialized before training.\")\n",
        "\n",
        "        print(\"Configuring Training Arguments...\")\n",
        "        training_args = TrainingArguments(**TRAINING_ARGS_CONFIG)\n",
        "\n",
        "        # Data collator dynamically pads batches to the longest sequence in the batch.\n",
        "        # mlm=False indicates standard causal language modeling (not masked language modeling).\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)\n",
        "\n",
        "        print(\"Initializing Trainer...\")\n",
        "        # Trainer integrates with Accelerator automatically for device placement and distributed training.\n",
        "        trainer = Trainer(\n",
        "            model=self.model, # The potentially PEFT-modified and quantized model\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            # You can add compute_metrics function here for custom evaluation metrics\n",
        "            # compute_metrics=compute_metrics_function,\n",
        "        )\n",
        "\n",
        "        # --- Start Training ---\n",
        "        print(\"Starting training process...\")\n",
        "        print(f\"Checkpoints will be saved to: {training_args.output_dir}\")\n",
        "        print(f\"Logs will be saved to: {training_args.logging_dir}\")\n",
        "\n",
        "        # Resume from checkpoint if exists\n",
        "        last_checkpoint = None\n",
        "        if os.path.isdir(training_args.output_dir):\n",
        "            last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "            if last_checkpoint:\n",
        "                print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
        "\n",
        "        train_result = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "        print(\"Training finished.\")\n",
        "\n",
        "        # --- Save Results ---\n",
        "        # Log & save training metrics\n",
        "        metrics = train_result.metrics\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state() # Saves optimizer state, scheduler state, etc.\n",
        "\n",
        "        # Save the final model (handles PEFT adapters correctly)\n",
        "        print(f\"Saving final model to {OUTPUT_DIR}...\")\n",
        "        # Ensure the final output directory exists\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        trainer.save_model(OUTPUT_DIR)\n",
        "        # Tokenizer is usually saved by trainer.save_model, but save explicitly if needed\n",
        "        if not os.path.exists(os.path.join(OUTPUT_DIR, 'tokenizer_config.json')):\n",
        "            self.tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "            print(f\"Tokenizer explicitly saved to {OUTPUT_DIR}\")\n",
        "\n",
        "        print(f\"Best model checkpoint was: {trainer.state.best_model_checkpoint}\")\n",
        "        print(f\"Final model artifacts (adapter/full model) saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "        return trainer\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"Executes the complete fine-tuning workflow.\"\"\"\n",
        "        try:\n",
        "            # 1. Load Data\n",
        "            df = self.load_excel_data(EXCEL_FILE_PATH)\n",
        "\n",
        "            # 2. Create Prompts\n",
        "            training_prompts = self.create_training_prompts(df)\n",
        "            if not training_prompts:\n",
        "                print(\"Error: No training prompts generated. Check data or prompt creation logic. Aborting.\")\n",
        "                return\n",
        "\n",
        "            # 3. Initialize Model and Tokenizer (with CUDA/Quantization/LoRA handling)\n",
        "            self.initialize_model_and_tokenizer()\n",
        "\n",
        "            # 4. Prepare Datasets\n",
        "            train_dataset, val_dataset = self.prepare_datasets(training_prompts)\n",
        "\n",
        "            # 5. Train Model\n",
        "            self.train(train_dataset, val_dataset)\n",
        "\n",
        "            print(\"\\n--- Fine-tuning pipeline completed successfully! ---\")\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"\\n--- Pipeline failed: {e} ---\")\n",
        "        except ValueError as e:\n",
        "            print(f\"\\n--- Pipeline failed due to value error: {e} ---\")\n",
        "        except ImportError as e:\n",
        "             print(f\"\\n--- Pipeline failed due to missing library: {e} ---\")\n",
        "             print(\"Please ensure all required libraries (torch, transformers, pandas, accelerate, peft, bitsandbytes, etc.) are installed.\")\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            print(\"\\n--- Pipeline failed: CUDA Out of Memory! ---\")\n",
        "            print(\"Try reducing `per_device_train_batch_size`, increasing `gradient_accumulation_steps`, enabling LoRA/Quantization, or using a smaller model.\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n--- An unexpected error occurred: {e} ---\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# --- Main Execution Block ---"
      ],
      "metadata": {
        "id": "KPxIaiempTkV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjCY47KnppXu",
        "outputId": "db722425-60f2-4a1a-c54e-489d527627fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "0b3be3aadaaf4ae3b3eaf22e2e76dacd",
            "f1011f8f839d441c8bdc247bc1f4c83b",
            "863545487a794d6fa2d276eef6a51c9c",
            "b3fe5740adba42c2a1f6149b4a52828d",
            "b2015f60194542149372bd77953bd995",
            "64400b2371ba457e85ea19ff1a8465b1",
            "eb513e502c8441f6b788eb7a2c06356f",
            "a91c3bf55e7a4d7b882aa1f35ff87ae6",
            "be05c6f9af0a4e988e9773cebb2b4f58",
            "816fc3862bc54179861f9dc93b11c4c5",
            "c4208f4bb56a4f218ccf10ce22d3fad5",
            "2ece87ed68684b1b9a3eca6feb6c9559",
            "59d4105c7f3c434ebeaf94f2c0d0fd43",
            "3d2e33aa51c24ba3ab6bf216373a5fd9",
            "bde7a41752e84266a6b9382fbf2c299e",
            "392ca13cc3584643b1ca2a472c088859",
            "5e16aa6089d343a6848ec5287cb9e6b7",
            "350a0d45d30846f09293f92ed05d474f",
            "356e395eeaf34c1ba6574f2385ba5c7d",
            "808924d9eff94764845e8e4fd2238460"
          ]
        },
        "id": "ucbxwXNeqHtq",
        "outputId": "6c946515-46d6-4285-b745-e01069300d61"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b3be3aadaaf4ae3b3eaf22e2e76dacd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"=====================================================\")\n",
        "    print(\" Starting Resource Skills Fine-Tuning Pipeline (CUDA Optimized) \")\n",
        "    print(\"=====================================================\")\n",
        "\n",
        "    # Ensure necessary directories exist\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(LOGGING_DIR, exist_ok=True)\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "    print(f\"Logging directory: {LOGGING_DIR}\")\n",
        "    print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
        "    print(f\"Using base model: {BASE_MODEL_NAME}\")\n",
        "    print(f\"Using LoRA: {USE_LORA}\")\n",
        "    print(f\"Using Quantization: {USE_QUANTIZATION}\")\n",
        "    print(\"-----------------------------------------------------\")\n",
        "\n",
        "    # Create and run the pipeline\n",
        "    finetuner = ResourceSkillsFineTuner(\n",
        "        model_name=BASE_MODEL_NAME,\n",
        "        use_lora=USE_LORA,\n",
        "        quantization_config=QUANTIZATION_CONFIG\n",
        "    )\n",
        "    finetuner.run_pipeline()\n",
        "\n",
        "    print(\"=====================================================\")\n",
        "    print(\" Pipeline execution finished. \")\n",
        "    print(\"=====================================================\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763,
          "referenced_widgets": [
            "6bd5bbcbe4b94342b667a32411a51fb5",
            "fc37dde6c67847f083cf94e621817fb9",
            "e904cec44e9f4006b3364e32cba78705",
            "246b3ff89747485c84420205a95de18b",
            "8aad9ba53e29457fb604e915b7cfc741",
            "441f3a85bddd47f8af4a93ab77b14278",
            "09e0a60bc8624527bcca1ae4b6de9696",
            "8429c4484c8346d092b8ca06f2d69b41",
            "857b7706a701414a832755cf21227a40",
            "9e7bd2b7e4f04d67bebfc3cbe059ab16",
            "2cfbdaedbb2a4db7817952dd8cda588f"
          ]
        },
        "id": "Lv3ZRpk-pZ0w",
        "outputId": "0e3aaa3f-3414-45d8-b484-9ee2b41c3cc6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================\n",
            " Starting Resource Skills Fine-Tuning Pipeline (CUDA Optimized) \n",
            "=====================================================\n",
            "Output directory: ./output/finetuned_resource_model_cuda\n",
            "Logging directory: ./logs/finetuning_logs_cuda\n",
            "Checkpoint directory: ./checkpoint/finetuning_checkpoints_cuda\n",
            "Using base model: mistralai/Mistral-7B-Instruct-v0.1\n",
            "Using LoRA: True\n",
            "Using Quantization: True\n",
            "-----------------------------------------------------\n",
            "Loading Excel file from: ./Resource-Skills-Experience-Data2.xlsx\n",
            "Loaded 688 records. Columns: ['Name', 'Business Unit', 'Department', 'Role', 'Job Title', 'Job Title/Role', 'Country Code', 'Resource Worker Type (employee/contractor)', 'Line Manager', 'Languages', 'Resource ID', 'Degree(s)', 'Therapeutic Indication', 'Therapeutic Area', 'Company experience (years)', 'Speciality', 'Industry experience (years)', 'Pediatric Experience (yes/no)', 'Regulatory/Technological Area', 'Drug Modality and Technological Area', 'Study Area', 'Service Area', 'Technical Writing Experience', 'Other Experience', 'Rate per hour', 'Resource availability  in Mar 25', 'Resource availability  in Apr 25', 'Resource availability  in May 25', 'Resource availability  in Jun 25', 'Resource availability  in Jul 25', 'Resource availability  in Aug 25', 'Resource availability  in Sep 25', 'Resource availability  in Oct 25', 'Resource availability  in Nov 25', 'Resource availability  in Dec 25', 'Resource availability  in Jan 26', 'Resource availability  in Feb 26']\n",
            "Creating training prompts from DataFrame...\n",
            "Warning: Missing expected columns in Excel file: ['Job Title/Role Group', 'Org Unit L3', 'Org Unit L4', 'Country', 'Worker Type', 'Manager Name', 'Total Experience (years)', 'Degree', 'Functional Expertise', 'Technical Skills', 'Certifications', 'Key Strengths', 'Current Allocation (%)', 'Availability (%)', 'On Bench']\n",
            "Generated 2064 raw training examples.\n",
            "Returning 2064 unique training examples after deduplication.\n",
            "Loading tokenizer: mistralai/Mistral-7B-Instruct-v0.1\n",
            "Tokenizer pad_token set to eos_token (</s>)\n",
            "Loading model: mistralai/Mistral-7B-Instruct-v0.1\n",
            "Applying quantization config...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bd5bbcbe4b94342b667a32411a51fb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing model for k-bit training (post-quantization setup)...\n",
            "Applying LoRA configuration...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA enabled. Trainable parameters:\n",
            "trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n",
            "Model and tokenizer initialized successfully.\n",
            "Splitting 2064 examples into train/validation sets (Test size: 0.1)...\n",
            "Train set size: 1857, Validation set size: 207\n",
            "Initialized dataset with 1857 examples.\n",
            "Initialized dataset with 207 examples.\n",
            "Configuring Training Arguments...\n",
            "Initializing Trainer...\n",
            "Starting training process...\n",
            "Checkpoints will be saved to: ./checkpoint/finetuning_checkpoints_cuda\n",
            "Logs will be saved to: ./logs/finetuning_logs_cuda\n",
            "\n",
            "--- Pipeline failed due to value error: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected). ---\n",
            "=====================================================\n",
            " Pipeline execution finished. \n",
            "=====================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b3be3aadaaf4ae3b3eaf22e2e76dacd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_eb513e502c8441f6b788eb7a2c06356f"
          }
        },
        "f1011f8f839d441c8bdc247bc1f4c83b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a91c3bf55e7a4d7b882aa1f35ff87ae6",
            "placeholder": "​",
            "style": "IPY_MODEL_be05c6f9af0a4e988e9773cebb2b4f58",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "863545487a794d6fa2d276eef6a51c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_816fc3862bc54179861f9dc93b11c4c5",
            "placeholder": "​",
            "style": "IPY_MODEL_c4208f4bb56a4f218ccf10ce22d3fad5",
            "value": ""
          }
        },
        "b3fe5740adba42c2a1f6149b4a52828d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_2ece87ed68684b1b9a3eca6feb6c9559",
            "style": "IPY_MODEL_59d4105c7f3c434ebeaf94f2c0d0fd43",
            "value": true
          }
        },
        "b2015f60194542149372bd77953bd995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3d2e33aa51c24ba3ab6bf216373a5fd9",
            "style": "IPY_MODEL_bde7a41752e84266a6b9382fbf2c299e",
            "tooltip": ""
          }
        },
        "64400b2371ba457e85ea19ff1a8465b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_392ca13cc3584643b1ca2a472c088859",
            "placeholder": "​",
            "style": "IPY_MODEL_5e16aa6089d343a6848ec5287cb9e6b7",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "eb513e502c8441f6b788eb7a2c06356f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "a91c3bf55e7a4d7b882aa1f35ff87ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be05c6f9af0a4e988e9773cebb2b4f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "816fc3862bc54179861f9dc93b11c4c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4208f4bb56a4f218ccf10ce22d3fad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ece87ed68684b1b9a3eca6feb6c9559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59d4105c7f3c434ebeaf94f2c0d0fd43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d2e33aa51c24ba3ab6bf216373a5fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bde7a41752e84266a6b9382fbf2c299e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "392ca13cc3584643b1ca2a472c088859": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e16aa6089d343a6848ec5287cb9e6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "350a0d45d30846f09293f92ed05d474f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_356e395eeaf34c1ba6574f2385ba5c7d",
            "placeholder": "​",
            "style": "IPY_MODEL_808924d9eff94764845e8e4fd2238460",
            "value": "Connecting..."
          }
        },
        "356e395eeaf34c1ba6574f2385ba5c7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "808924d9eff94764845e8e4fd2238460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bd5bbcbe4b94342b667a32411a51fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc37dde6c67847f083cf94e621817fb9",
              "IPY_MODEL_e904cec44e9f4006b3364e32cba78705",
              "IPY_MODEL_246b3ff89747485c84420205a95de18b"
            ],
            "layout": "IPY_MODEL_8aad9ba53e29457fb604e915b7cfc741"
          }
        },
        "fc37dde6c67847f083cf94e621817fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_441f3a85bddd47f8af4a93ab77b14278",
            "placeholder": "​",
            "style": "IPY_MODEL_09e0a60bc8624527bcca1ae4b6de9696",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e904cec44e9f4006b3364e32cba78705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8429c4484c8346d092b8ca06f2d69b41",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_857b7706a701414a832755cf21227a40",
            "value": 2
          }
        },
        "246b3ff89747485c84420205a95de18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e7bd2b7e4f04d67bebfc3cbe059ab16",
            "placeholder": "​",
            "style": "IPY_MODEL_2cfbdaedbb2a4db7817952dd8cda588f",
            "value": " 2/2 [01:35&lt;00:00, 44.39s/it]"
          }
        },
        "8aad9ba53e29457fb604e915b7cfc741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "441f3a85bddd47f8af4a93ab77b14278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09e0a60bc8624527bcca1ae4b6de9696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8429c4484c8346d092b8ca06f2d69b41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "857b7706a701414a832755cf21227a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e7bd2b7e4f04d67bebfc3cbe059ab16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cfbdaedbb2a4db7817952dd8cda588f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}